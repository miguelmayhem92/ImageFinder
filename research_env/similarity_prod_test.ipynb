{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal of this notebook is to explore more in depth a solution that:\n",
    "* store a dataset of images\n",
    "* explore further similarity metrics\n",
    "* explore dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from  mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow 2.1.1\n",
      "torch 2.0.0+cpu\n",
      "torchvision 0.15.1+cpu\n",
      "datasets 2.12.0\n",
      "numpy 1.21.5\n"
     ]
    }
   ],
   "source": [
    "import mlflow as mlf\n",
    "print('mlflow' + ' ' + mlf.__version__)\n",
    "import torch as trc\n",
    "print('torch' + ' ' + trc.__version__)\n",
    "import torchvision as trv\n",
    "print('torchvision' + ' ' + trv.__version__)\n",
    "import datasets as dts\n",
    "print('datasets' + ' ' + dts.__version__)\n",
    "import numpy as np\n",
    "print('numpy' + ' ' + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_local_path = os.path.normpath(os.getcwd() + os.sep + os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_name = 'ImageFinder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = f'{proj_name}_models' \n",
    "# model credential\n",
    "uri = 'runs:/1f30b69e008e42c795b551e3fb240884'\n",
    "tmp_path = my_local_path + '/research_env/tmp_image/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Loading Model and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_dict = mlflow.artifacts.load_dict(\n",
    "    uri+'/extractor_dict.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1060cc05f7c4297bc5795a9baa38146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5031f2f5ef6f4fa1a5251f917e81d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/Miguel/.cache/huggingface/datasets/imagefolder/default-9098521b0ec6eb33/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd862f41df54abd985df22c0f79d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = my_local_path + '/dataset'\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=image_path, drop_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation chain.\n",
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize(int((224 / 224) * extractor_dict[\"size\"][\"height\"])),\n",
    "        T.CenterCrop(extractor_dict[\"size\"][\"height\"]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=extractor_dict[\"image_mean\"], std=extractor_dict[\"image_std\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = mlflow.artifacts.download_artifacts(\n",
    "            run_id= '7a8aae8b4b91454fb177ad796c95cf5b',\n",
    "            artifact_path=f\"{proj_name}-run\"\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pytorch.load_model(model_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_db = torch.load('embeding_db.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_loaded = embeding_db[:,0:-1]\n",
    "ids_loaded = embeding_db[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b145d84235484795ae283edd022950cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "candidate_ids = []\n",
    "\n",
    "for id in tqdm(range(len(ids_loaded))):\n",
    "    id_true = int(np.array(ids_loaded[id]))\n",
    "\n",
    "    # Create a unique indentifier.\n",
    "    entry = str(id) + \"_\" + str(id_true)\n",
    "\n",
    "    candidate_ids.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)\n",
    "    return scores.numpy().tolist()\n",
    "\n",
    "\n",
    "def fetch_similar(model, all_candidate_embeddings, image, top_k=5):\n",
    "    \"\"\"Fetches the `top_k` similar images with `image` as the query.\"\"\"\n",
    "    # Prepare the input query image for embedding computation.\n",
    "    image_transformed = transformation_chain(image).unsqueeze(0)\n",
    "    new_batch = {\"pixel_values\": image_transformed.to(device)}\n",
    "\n",
    "    # Comute the embedding.\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "    # Compute similarity scores with all the candidate images at one go.\n",
    "    # We also create a mapping between the candidate image identifiers\n",
    "    # and their similarity scores with the query image.\n",
    "    sim_scores = compute_scores(all_candidate_embeddings, query_embeddings)\n",
    "    similarity_mapping = dict(zip(candidate_ids, sim_scores))\n",
    " \n",
    "    # Sort the mapping dictionary and return `top_k` candidates.\n",
    "    similarity_mapping_sorted = dict(\n",
    "        sorted(similarity_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "    id_entries = list(similarity_mapping_sorted.keys())[:top_k]\n",
    "\n",
    "    ids = list(map(lambda x: int(x.split(\"_\")[0]), id_entries))\n",
    "    ids_true = list(map(lambda x: int(x.split(\"_\")[-1]), id_entries))\n",
    "    scores = list(similarity_mapping_sorted.values())[:top_k]\n",
    "    \n",
    "    return ids, ids_true, scores, similarity_mapping_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test id is: 38\n",
      "top 5 ids are: [6, 80, 44, 20, 111]\n",
      "top 5 actual ids are: [423485, 106187, 579115, 117201, 195166]\n",
      "top 5 scores are: [0.6933450088041885, 0.6407462925440754, 0.5778804254596666, 0.4826035706874567, 0.44418710175889053]\n"
     ]
    }
   ],
   "source": [
    "#test_idx = np.random.choice(len(dataset[\"test\"]))  ## good ids 72, 38, 17\n",
    "test_idx = 38\n",
    "test_sample = dataset[\"test\"][test_idx][\"image\"]\n",
    "\n",
    "sim_ids, sim_ids_true, sim_score, sim_map = fetch_similar(model, embeddings_loaded ,test_sample)\n",
    "print(f'test id is: {test_idx}')\n",
    "print(f\"top 5 ids are: {sim_ids}\")\n",
    "print(f\"top 5 actual ids are: {sim_ids_true}\")\n",
    "print(f\"top 5 scores are: {sim_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to C:/Users/Miguel/.cache/huggingface/datasets/imagefolder/default-c95b25725be68847/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddc758ba63b40998c328d0313fcd572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d04c888a71402da1c67bf55a501398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce458a0ceda427d9fecbbea776c2c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to C:/Users/Miguel/.cache/huggingface/datasets/imagefolder/default-c95b25725be68847/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911dd72dd32341a39907e593fa89023e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test id is: 38\n",
      "top 5 ids are: [6, 80, 44, 20, 111]\n",
      "top 5 actual ids are: [423485, 106187, 579115, 117201, 195166]\n",
      "top 5 scores are: [0.6933450088041885, 0.6407462925440754, 0.5778804254596666, 0.4826035706874567, 0.44418710175889053]\n"
     ]
    }
   ],
   "source": [
    "tmp_image_name = '303217440_f595f9b310_o.jpg'\n",
    "test_id_path = my_local_path + '/dataset/test/' + tmp_image_name\n",
    "\n",
    "if not os.path.exists(tmp_path):\n",
    "        os.makedirs(tmp_path)\n",
    "        \n",
    "dst = tmp_path + tmp_image_name\n",
    "shutil.copyfile(test_id_path, dst)\n",
    "\n",
    "tmp_dataset = load_dataset(\"imagefolder\", data_dir=tmp_path, drop_labels=True)\n",
    "\n",
    "tmp_image = tmp_dataset['train'][0]['image']\n",
    "\n",
    "sim_ids, sim_ids_true, sim_score, sim_map = fetch_similar(model, embeddings_loaded,tmp_image)\n",
    "print(f'test id is: {test_idx}')\n",
    "print(f\"top 5 ids are: {sim_ids}\")\n",
    "print(f\"top 5 actual ids are: {sim_ids_true}\")\n",
    "print(f\"top 5 scores are: {sim_score}\")\n",
    "\n",
    "shutil.rmtree(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{423485: 0.6933450088041885,\n",
       " 106187: 0.6407462925440754,\n",
       " 579115: 0.5778804254596666,\n",
       " 117201: 0.4826035706874567,\n",
       " 195166: 0.44418710175889053}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(sim_ids_true, sim_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
